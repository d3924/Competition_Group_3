---
title: "competition"
author: "Jinghao Xu"
date: '2022-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Library

```{r cars}
competition_data <- read.csv("competition-data.csv", header = TRUE, sep = "," )
library(caret)
library(lattice)
library(pls)
library(tidyverse)
library(elasticnet)
library(corrplot)
library(tidyverse)
library(caret)
library(modelr)
library(ggplot2)
library(dplyr)
library(caTools)
library(RWeka)
library(earth)
```

## Exploration

first we need to explore the distribution of the outcome.

```{r}
competition_data %>%
  ggplot(mapping = aes(x=outcome))+geom_bar()
```
We can find that the outcome is not normal. we need to preprocess the data to make it "center" and "scale".

## Preprocess

we need to extract the outcome as vector and eliminate high correlation coefficients.  

```{r}
##extract outcome
outcome=c(competition_data$outcome)
train=competition_data %>% select(-outcome)
## reduce noise
tooHigh <- findCorrelation(cor(train), cutoff = .9)
low_noise_train <- train[, -tooHigh]
```

## cross validation initialize

```{r}
## to improve accuracy, we will use k=10.
folds <- createFolds(outcome, k = 10, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = folds)
```


We will try to use several different method to predict the outcome. Since we have no need to consider the complexity and running time of our models, We will check the "RMSE" value to determine which model to choose.

## 1 linear
```{r}
 set.seed(100)
  lmTuneFiltered <- train(x = low_noise_train, y = outcome,
                  method = "lm",
                  trControl = ctrl,
                  preProc = c("center", "scale")
                  )

```

```{r}
lmTuneFiltered$results$RMSE
```

## 2 Ridge
```{r}
  set.seed(100)
  ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 10))
  ridgeTune <- train(x = low_noise_train, y = outcome,
                     method = "ridge",
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProc = c("center", "scale")
                )
```

```{r}
min(ridgeTune$results$RMSE)
ggplot(ridgeTune)
```
## 3 Lasso
```{r}
 set.seed(100)
  lassoGrid <- expand.grid(lambda = c(0), 
                        fraction = seq(.1, 1, length = 15))
  lassoTune <- train(x = low_noise_train, y = outcome,
                      method = "enet",
                      tuneGrid = lassoGrid,
                      trControl = ctrl,
                      preProc = c("center", "scale")
                    )
```

```{r}
min(lassoTune$results$RMSE)
ggplot(lassoTune)
```
We can find that the linear methods do not fit this dataset very well. The RMSE are all around 10.

### 4 KNN

```{r}
svmRTuned <- train(
    low_noise_train, outcome, 
    method = "svmRadial",
    preProc = c("center", "scale"),
    tuneLength = 8,
    epsilon = 0.01,
    trControl = ctrl
  )
```

```{r}
min(svmRTuned$results$RMSE)
ggplot(svmRTuned)
```
### 5 One layer neural network 
```{r}
set.seed(100)
   nnetGrid <- expand.grid(
    decay = seq(0,100,length=10), 
    size = c(3) 
  )  
  nnetTune <- train(low_noise_train, outcome,
                     method = "nnet",
                     tuneGrid = nnetGrid,
                     trControl = ctrl,
                     preProc = c("center", "scale"),
                     linout = TRUE, 
                     trace = FALSE
  )
  
```

```{r}
min(nnetTune$results$RMSE)
ggplot(nnetTune)
```

### 6 Mars
```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = c(20, 30, 40, 50))
    marsFit <- train(low_noise_train, outcome,
                       method = "earth",
                       tuneGrid = marsGrid,
                       trControl = ctrl
    )
```

```{r}
min(marsFit$results$RMSE)
ggplot(marsFit)

```


### 7 linear SVM
```{r}
 svmLinearTuned <- train(
    low_noise_train, outcome, 
    method = "svmLinear",
    preProc = c("center", "scale"),
    tuneLength = 8,
    epsilon = 0.01,
    trControl = ctrl
  )
```

```{r}
svmLinearTuned$results$RMSE
```
The result is very surprising that they varies greatly. "Mars" seems to have the greatest result. Next we will try to use  "Trees"

### Regression tree

```{r}
set.seed(100)
m5Tune <- train(low_noise_train, outcome,
                 method = "M5",
                 trControl = ctrl,
                 control = Weka_control(M = 70),
)
```

```{r}
min(m5Tune$results$RMSE)
```

### Random forests

```{r}
mtryGrid <- data.frame(
  mtry = floor(seq(10, ncol(low_noise_train), 
                      length = 10))
  )
set.seed(100)
  rfTune <- train(x = low_noise_train, y = outcome,
                   method = "rf",
                   tuneGrid = mtryGrid,
                   ntree = 50,
                   importance = TRUE,
                   preProc = c("center", "scale"),
                   trControl = ctrl)
```

```{r}
min(rfTune$results$RMSE)
ggplot(rfTune)
```

Tree seems to run great on this model. We can find that random forest have the smallerst RMSE, and thats what we will choose.


## Prediction

```{r}
competition_test <- read.csv("competition-test-x-values.csv", header = TRUE, sep = "," )
predictions<-predict(rfTune,newdata = competition_test)
write.csv(predictions,"prediction.csv")
```

## Conclusion

We are actually not very satisfied with the final results. To improve it, we should have some further exploration on the relationship between each variables. In addition, the parameters should be adjusted to fit a better model.


